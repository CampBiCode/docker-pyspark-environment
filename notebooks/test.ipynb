{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c97cd2-b72d-4624-869d-a323278bd9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a14989-0ff5-4653-a8d8-0d586980d8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/11 15:41:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Practice environment\").master(\"spark://spark-master:7077\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cce1d7b9-63d6-4cae-8fee-1e39816e7978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_sales = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"data/vgsales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c79ce5a-8745-44eb-ab2c-04f51d2e34de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('Rank', IntegerType(), True), StructField('Name', StringType(), True), StructField('Platform', StringType(), True), StructField('Year', StringType(), True), StructField('Genre', StringType(), True), StructField('Publisher', StringType(), True), StructField('NA_Sales', DoubleType(), True), StructField('EU_Sales', DoubleType(), True), StructField('JP_Sales', DoubleType(), True), StructField('Other_Sales', DoubleType(), True), StructField('Global_Sales', DoubleType(), True)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b2695b2-e817-4793-bfc4-b33a900a4ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------+----+------------+--------------------+--------+--------+--------+-----------+------------+\n",
      "|Rank|                Name|Platform|Year|       Genre|           Publisher|NA_Sales|EU_Sales|JP_Sales|Other_Sales|Global_Sales|\n",
      "+----+--------------------+--------+----+------------+--------------------+--------+--------+--------+-----------+------------+\n",
      "|   1|          Wii Sports|     Wii|2006|      Sports|            Nintendo|   41.49|   29.02|    3.77|       8.46|       82.74|\n",
      "|   2|   Super Mario Bros.|     NES|1985|    Platform|            Nintendo|   29.08|    3.58|    6.81|       0.77|       40.24|\n",
      "|   3|      Mario Kart Wii|     Wii|2008|      Racing|            Nintendo|   15.85|   12.88|    3.79|       3.31|       35.82|\n",
      "|   4|   Wii Sports Resort|     Wii|2009|      Sports|            Nintendo|   15.75|   11.01|    3.28|       2.96|        33.0|\n",
      "|   5|Pokemon Red/Pokem...|      GB|1996|Role-Playing|            Nintendo|   11.27|    8.89|   10.22|        1.0|       31.37|\n",
      "|   6|              Tetris|      GB|1989|      Puzzle|            Nintendo|    23.2|    2.26|    4.22|       0.58|       30.26|\n",
      "|   7|New Super Mario B...|      DS|2006|    Platform|            Nintendo|   11.38|    9.23|     6.5|        2.9|       30.01|\n",
      "|   8|            Wii Play|     Wii|2006|        Misc|            Nintendo|   14.03|     9.2|    2.93|       2.85|       29.02|\n",
      "|   9|New Super Mario B...|     Wii|2009|    Platform|            Nintendo|   14.59|    7.06|     4.7|       2.26|       28.62|\n",
      "|  10|           Duck Hunt|     NES|1984|     Shooter|            Nintendo|   26.93|    0.63|    0.28|       0.47|       28.31|\n",
      "|  11|          Nintendogs|      DS|2005|  Simulation|            Nintendo|    9.07|    11.0|    1.93|       2.75|       24.76|\n",
      "|  12|       Mario Kart DS|      DS|2005|      Racing|            Nintendo|    9.81|    7.57|    4.13|       1.92|       23.42|\n",
      "|  13|Pokemon Gold/Poke...|      GB|1999|Role-Playing|            Nintendo|     9.0|    6.18|     7.2|       0.71|        23.1|\n",
      "|  14|             Wii Fit|     Wii|2007|      Sports|            Nintendo|    8.94|    8.03|     3.6|       2.15|       22.72|\n",
      "|  15|        Wii Fit Plus|     Wii|2009|      Sports|            Nintendo|    9.09|    8.59|    2.53|       1.79|        22.0|\n",
      "|  16|  Kinect Adventures!|    X360|2010|        Misc|Microsoft Game St...|   14.97|    4.94|    0.24|       1.67|       21.82|\n",
      "|  17|  Grand Theft Auto V|     PS3|2013|      Action|Take-Two Interactive|    7.01|    9.27|    0.97|       4.14|        21.4|\n",
      "|  18|Grand Theft Auto:...|     PS2|2004|      Action|Take-Two Interactive|    9.43|     0.4|    0.41|      10.57|       20.81|\n",
      "|  19|   Super Mario World|    SNES|1990|    Platform|            Nintendo|   12.78|    3.75|    3.54|       0.55|       20.61|\n",
      "|  20|Brain Age: Train ...|      DS|2005|        Misc|            Nintendo|    4.75|    9.26|    4.16|       2.05|       20.22|\n",
      "+----+--------------------+--------+----+------------+--------------------+--------+--------+--------+-----------+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "267faba8-5b94-42e6-9f86-ccda5d40425c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Rank: int, Name: string, Platform: string, Year: string, Genre: string, Publisher: string, NA_Sales: double, EU_Sales: double, JP_Sales: double, Other_Sales: double, Global_Sales: double]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6098209b-b655-4a54-9816-c39271edee20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Year| sum(Global_Sales)|\n",
      "+----+------------------+\n",
      "|1987|21.739999999999995|\n",
      "|2016| 70.93000000000013|\n",
      "|2012| 363.5399999999984|\n",
      "|2020|              0.29|\n",
      "|1988|             47.22|\n",
      "|2017|              0.05|\n",
      "|2014| 337.0499999999985|\n",
      "|1984|50.360000000000014|\n",
      "|2013|368.10999999999865|\n",
      "|1982|28.859999999999996|\n",
      "|2005| 459.9399999999976|\n",
      "|2000| 201.5600000000002|\n",
      "|1981| 35.77000000000001|\n",
      "|2002| 395.5199999999983|\n",
      "|2009| 667.2999999999947|\n",
      "|1995| 88.10999999999991|\n",
      "|2006| 521.0399999999917|\n",
      "|2004|419.30999999999864|\n",
      "|1989|             73.45|\n",
      "|2011| 515.9899999999969|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_sales.groupBy(\"Year\").sum(\"Global_Sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b682820d-aa4b-489b-955f-55f59d47ac00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 25.0 in stage 13.0 (TID 256) (172.18.0.5 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "26/01/11 15:43:31 ERROR TaskSetManager: Task 57 in stage 13.0 failed 4 times; aborting job\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 59.1 in stage 13.0 (TID 353) (172.18.0.4 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 6.2 in stage 13.0 (TID 360) (172.18.0.4 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 5.1 in stage 13.0 (TID 298) (172.18.0.3 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 9.3 in stage 13.0 (TID 406) (172.18.0.3 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 39.2 in stage 13.0 (TID 354) (172.18.0.4 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 11.2 in stage 13.0 (TID 404) (172.18.0.3 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 56.2 in stage 13.0 (TID 408) (172.18.0.5 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 46.2 in stage 13.0 (TID 347) (172.18.0.5 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 17.2 in stage 13.0 (TID 403) (172.18.0.3 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 18.1 in stage 13.0 (TID 351) (172.18.0.4 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 24.1 in stage 13.0 (TID 346) (172.18.0.5 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 33.1 in stage 13.0 (TID 345) (172.18.0.5 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 1.3 in stage 13.0 (TID 398) (172.18.0.3 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 43.2 in stage 13.0 (TID 348) (172.18.0.4 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 28.2 in stage 13.0 (TID 339) (172.18.0.5 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 48.1 in stage 13.0 (TID 340) (172.18.0.4 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 52.2 in stage 13.0 (TID 392) (172.18.0.3 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 42.2 in stage 13.0 (TID 388) (172.18.0.3 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 37.1 in stage 13.0 (TID 338) (172.18.0.5 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 47.1 in stage 13.0 (TID 335) (172.18.0.5 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 29.2 in stage 13.0 (TID 381) (172.18.0.3 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 30.2 in stage 13.0 (TID 344) (172.18.0.4 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 12.1 in stage 13.0 (TID 337) (172.18.0.4 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 13.1 in stage 13.0 (TID 332) (172.18.0.5 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 3.3 in stage 13.0 (TID 382) (172.18.0.3 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 22.3 in stage 13.0 (TID 400) (172.18.0.4 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 40.2 in stage 13.0 (TID 383) (172.18.0.3 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 14.3 in stage 13.0 (TID 405) (172.18.0.4 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 32.2 in stage 13.0 (TID 407) (172.18.0.5 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 20.2 in stage 13.0 (TID 399) (172.18.0.5 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 38.3 in stage 13.0 (TID 372) (172.18.0.3 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 44.2 in stage 13.0 (TID 397) (172.18.0.5 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 34.3 in stage 13.0 (TID 401) (172.18.0.4 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 10.2 in stage 13.0 (TID 370) (172.18.0.3 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 45.3 in stage 13.0 (TID 402) (172.18.0.4 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 23.3 in stage 13.0 (TID 396) (172.18.0.5 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 55.2 in stage 13.0 (TID 371) (172.18.0.3 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 49.3 in stage 13.0 (TID 395) (172.18.0.4 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 26.2 in stage 13.0 (TID 393) (172.18.0.4 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 51.3 in stage 13.0 (TID 367) (172.18.0.3 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 41.2 in stage 13.0 (TID 391) (172.18.0.5 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 31.2 in stage 13.0 (TID 358) (172.18.0.3 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 27.2 in stage 13.0 (TID 390) (172.18.0.4 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 50.2 in stage 13.0 (TID 387) (172.18.0.5 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 54.1 in stage 13.0 (TID 350) (172.18.0.3 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 25.2 in stage 13.0 (TID 386) (172.18.0.5 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 36.3 in stage 13.0 (TID 389) (172.18.0.4 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 58.2 in stage 13.0 (TID 379) (172.18.0.4 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 7.2 in stage 13.0 (TID 384) (172.18.0.5 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 15.2 in stage 13.0 (TID 355) (172.18.0.3 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n",
      "26/01/11 15:43:31 WARN TaskSetManager: Lost task 0.2 in stage 13.0 (TID 378) (172.18.0.5 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n",
      "    check_python_version(infile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n    check_python_version(infile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\n\tat scala.collection.immutable.List.foreach(List.scala:323)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1017)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2496)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2517)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2536)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2561)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:205)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n    check_python_version(infile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TaskContext.get().executorId()\n\u001b[32m      9\u001b[39m df = spark.range(\u001b[32m0\u001b[39m, \u001b[32m100\u001b[39m)\n\u001b[32m     11\u001b[39m executors = (\n\u001b[32m     12\u001b[39m     \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrdd\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m      \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere_am_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m      \u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistinct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43m      \u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m executors\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/core/rdd.py:1700\u001b[39m, in \u001b[36mRDD.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1698\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m.context):\n\u001b[32m   1699\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1700\u001b[39m     sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonRDD\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1701\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m._jrdd_deserializer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:263\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    265\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 57 in stage 13.0 failed 4 times, most recent failure: Lost task 57.3 in stage 13.0 (TID 394) (172.18.0.3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n    check_python_version(infile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\n\tat scala.collection.immutable.List.foreach(List.scala:323)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1017)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2496)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2517)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2536)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2561)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:205)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3305, in main\n    check_python_version(infile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 77, in check_python_version\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version: 3.10 than that in driver: 3.11, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import TaskContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def where_am_i(_):\n",
    "    return TaskContext.get().executorId()\n",
    "\n",
    "df = spark.range(0, 100)\n",
    "\n",
    "executors = (\n",
    "    df.rdd\n",
    "      .mapPartitions(where_am_i)\n",
    "      .distinct()\n",
    "      .collect()\n",
    ")\n",
    "\n",
    "executors\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
